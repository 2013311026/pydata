{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import os\n",
    "\n",
    "# filePath = r'D:\\data'\n",
    "\n",
    "# fileMap = {}\n",
    "# size = 0\n",
    "\n",
    "# # 遍历filePath下的文件、文件夹（包括子目录）\n",
    "# for parent, dirnames, filenames in os.walk(filePath):\n",
    "#     for dirname in dirnames:\n",
    "# #         print('parent is %s, dirname is %s' % (parent, dirname))\n",
    "#         pass\n",
    "\n",
    "#     for filename in filenames:\n",
    "# #         print('parent is %s, filename is %s' % (parent, filename))\n",
    "# #         print('the full name of the file is %s' % os.path.join(parent, filename))\n",
    "        \n",
    "#         size = os.path.getsize(os.path.join(parent, filename))\n",
    "#         fileMap.setdefault(os.path.join(parent, filename), size)\n",
    "\n",
    "# filelist = sorted(fileMap.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def df():\n",
    "#     for filename, size in filelist[:400]:\n",
    "#         reader = pd.read_csv(filename, index_col=False, iterator=True)\n",
    "#         loop = True\n",
    "#         chunkSize = 2000000\n",
    "#         chunks = []\n",
    "#         while loop:\n",
    "#             try:\n",
    "#                 chunk = reader.get_chunk(chunkSize)\n",
    "#                 chunk = chunk[['Value']]#['2018-07-12 00:00:00':'2018-07-19 00:00:00']\n",
    "#                 col = filename.split('.')[-2].split('\\\\')[-1]\n",
    "#                 chunk.rename(columns={'Value':col}, inplace=True)\n",
    "#                 chunks.append(chunk)\n",
    "#             except StopIteration:\n",
    "#                 loop = False\n",
    "#                 print (\"Iteration is stopped.\")\n",
    "#         df = pd.concat(chunks, ignore_index=True, axis=0)\n",
    "#         yield df\n",
    "\n",
    "# dfs = pd.concat(df(), axis=1)\n",
    "\n",
    "# for filename, size in filelist[:1]:\n",
    "#     reader = pd.read_csv(filename, index_col=False)\n",
    "#     dfs.index = reader['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "\n",
    "# 读取并解析二进制文件\n",
    "def rbf(path):\n",
    "    fd = open(path, 'rb')\n",
    "    sigs = []\n",
    "    sigstime = []\n",
    "    while fd:\n",
    "        buf = fd.read(60)\n",
    "        if len(buf) == 0:\n",
    "            break\n",
    "\n",
    "        sig_status = struct.unpack('B',buf[:1])  # 烟支状态\n",
    "        spare_0 = struct.unpack('B',buf[1:2])\n",
    "        bo_number = struct.unpack('L',buf[2:6])  # 烟支序号\n",
    "        bo_time = struct.unpack('8H',buf[6:22])  # 烟支生产时间\n",
    "        bo_time = list(bo_time)\n",
    "        del bo_time[2]  # 删除本周的第几天\n",
    "        day = '-'.join([str(i) for i in bo_time[:3]])\n",
    "        hour = ':'.join([str(i) for i in bo_time[3:-1]])\n",
    "        bo_time = []\n",
    "        bo_time.append(day + ' ' + hour)  # 组合成'2018-12-14 8:18:28'的形式\n",
    "        bo_srm_Weight = struct.unpack('H',buf[22:24])  # 重量\n",
    "        bo_srm_xsd = struct.unpack('f',buf[24:28])  # 吸丝带位置\n",
    "        bo_cis = struct.unpack('6H',buf[28:40])  # 吸阻，通风度，漏气，松头，光学外观，圆周值\n",
    "        sig = sig_status+spare_0+bo_number+bo_srm_Weight+bo_srm_xsd+bo_cis\n",
    "        sig_time = bo_time\n",
    "        sigs.append(sig)\n",
    "        sigstime.append(sig_time)\n",
    "    fd.close()\n",
    "    return sigs, sigstime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 根据解析出来的列表数据生成DataFrame数据\n",
    "def gendf(path, rule='1s'):\n",
    "    sigs, sigstime = rbf(path)\n",
    "\n",
    "    df = pd.DataFrame(sigs)\n",
    "    df.columns = ['烟支状态','spare_0','烟支序号','重量','吸丝带位置','吸阻','通风度','漏气','松头','光学外观','圆周值']\n",
    "\n",
    "    time = pd.DataFrame(sigstime)\n",
    "    time.columns = ['生产时间']\n",
    "    time = pd.to_datetime(time.iloc[:,0])\n",
    "\n",
    "    df.index = time\n",
    "\n",
    "    df = df.resample(rule).first()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "filelist = [r'D:\\datasets\\宁波20190316\\高速数据二进制文件\\2019-3-6_7-30_224_liqun89.dat']\n",
    "dflist = []\n",
    "\n",
    "for f in filelist:\n",
    "    df = gendf(f)\n",
    "    dflist.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dflist[0].to_csv('data/dflist[0].csv', encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dflist[0].吸阻.plot(figsize=(12,6))\n",
    "plt.ylabel('吸阻')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = dflist[0].dropna().iloc[:,[3,4,5,6,7,8,9]]\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.吸阻.plot(figsize=(12,6))\n",
    "plt.ylabel('吸阻')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 基于3sigma的异常数据预警代码\n",
    "\n",
    "data_x = d.吸阻.index\n",
    "data_y = d.吸阻\n",
    "\n",
    "n = 3\n",
    "\n",
    "ymean = np.mean(data_y)\n",
    "ystd = np.std(data_y)\n",
    "threshold1 = ymean - n * ystd\n",
    "threshold2 = ymean + n * ystd\n",
    "\n",
    "outlier = [] #将异常值保存\n",
    "outlier_x = []\n",
    "\n",
    "for i in range(0, len(data_y)):\n",
    "    if (data_y[i] < threshold1)|(data_y[i] > threshold2):\n",
    "        outlier.append(data_y[i])\n",
    "        outlier_x.append(data_x[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "outlier = np.round(outlier, 3)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(data_x, data_y)\n",
    "plt.plot(outlier_x, outlier, 'ro')\n",
    "for j in range(len(outlier)):\n",
    "    plt.annotate(outlier[j], xy=(outlier_x[j], outlier[j]), xytext=(outlier_x[j],outlier[j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_y.drop(outlier_x).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于箱型图的异常数据预警代码\n",
    "\n",
    "n = 1.5\n",
    "\n",
    "statistics = data_y.describe() #保存基本统计量\n",
    "IQR = statistics.loc['75%']-statistics.loc['25%']   #四分位数间距\n",
    "QL = statistics.loc['25%']  #下四分位数\n",
    "QU = statistics.loc['75%']  #上四分位数\n",
    "threshold1 = QL - n*IQR #下阈值\n",
    "threshold2 = QU + n*IQR #上阈值\n",
    "outlier = [] #将异常值保存\n",
    "outlier_x = []\n",
    "\n",
    "for i in range(0, len(data_y)):\n",
    "    if (data_y[i] < threshold1)|(data_y[i] > threshold2):\n",
    "        outlier.append(data_y[i])\n",
    "        outlier_x.append(data_x[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "outlier = np.round(outlier, 3)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(data_x, data_y)\n",
    "plt.plot(outlier_x, outlier, 'ro')\n",
    "for j in range(len(outlier)):\n",
    "    plt.annotate(outlier[j], xy=(outlier_x[j], outlier[j]), xytext=(outlier_x[j],outlier[j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y.drop(outlier_x).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "x = data_y\n",
    "k2, p = stats.normaltest(x)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # 零假设: x来自正态分布\n",
    "    print(\"零假设能被拒绝\")\n",
    "else:\n",
    "    print(\"零假设不能被拒绝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.kstest(x, cdf='norm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def data_preprocessing(x, y):\n",
    "\n",
    "    X = x\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)\n",
    "\n",
    "    scaler1 = MinMaxScaler()\n",
    "\n",
    "    X_scaler1 = scaler1.fit_transform(X)\n",
    "    y_scaler1 = scaler1.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    scaler2 = StandardScaler()\n",
    "\n",
    "    X_scaler2 = scaler2.fit_transform(X)\n",
    "    y_scaler2 = scaler2.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X_scaler2, y_scaler2, test_size=0.2, random_state=666)\n",
    "    \n",
    "    return Xtrain, Xtest, ytrain, ytest, scaler2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso,Ridge,ElasticNet,RANSACRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def training_model(xtrain, ytrain, xtest, ytest):\n",
    "    # 训练回归模型\n",
    "    n_samples, n_features = xtrain.shape # 总样本数，总特征数\n",
    "    n_folds = 6 # 交叉检验次数\n",
    "\n",
    "    lr = LinearRegression() # 线性回归\n",
    "    svr = SVR() # 支持向量机回归\n",
    "    rf = RandomForestRegressor() # 随机森林回归\n",
    "    \"\"\"\n",
    "    n_estimators=20,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=n_features,\n",
    "    random_state=10\n",
    "    \"\"\"\n",
    "    gbr = GradientBoostingRegressor() # GBDT回归\n",
    "    \"\"\"\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=120,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    max_features=n_features,\n",
    "    subsample=1\n",
    "    \"\"\"\n",
    "    knn = KNeighborsRegressor() # K最近邻回归\n",
    "    \"\"\"\n",
    "    n_neighbors=5, \n",
    "    weights='distance', \n",
    "    leaf_size=50\n",
    "    \"\"\"\n",
    "    dt = DecisionTreeRegressor() # 决策树回归\n",
    "    \"\"\"\n",
    "    criterion='mse', \n",
    "    max_depth=None, \n",
    "    max_features=n_features, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, \n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split=2, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    random_state=None, \n",
    "    splitter='best'\n",
    "    \"\"\"\n",
    "    ransac = RANSACRegressor() # 随机采样一致性回归\n",
    "    ridge = Ridge() # Ridge回归\n",
    "    lasso = Lasso() # Lasso回归\n",
    "    en = ElasticNet() # 弹性网络回归\n",
    "\n",
    "    mnames = ['LinearRegression', \n",
    "              'SVR', \n",
    "              'RandomForestRegressor', \n",
    "              'GradientBoostingRegressor', \n",
    "              'KNeighborsRegressor', \n",
    "              'DecisionTreeRegressor', \n",
    "              'RANSACRegressor', \n",
    "              'Ridge', \n",
    "              'Lasso', \n",
    "              'ElasticNet'] # 回归模型名称列表\n",
    "    mlist = [lr, svr, rf, gbr, knn, dt, ransac, ridge, lasso, en] # 回归模型对象列表\n",
    "\n",
    "    cv_scores = [] # 交叉检验结果列表\n",
    "    y_pred = [] # 回归模型预测y值列表\n",
    "\n",
    "    for model in mlist:\n",
    "        scores = cross_val_score(model, xtrain, ytrain.ravel(), cv=n_folds) # 将每个回归模型导入交叉检验\n",
    "        predicted = model.fit(xtrain, ytrain.ravel()).predict(xtest) # 回归训练得到预测y值\n",
    "        cv_scores.append(scores) # 将交叉验证结果存入列表\n",
    "        y_pred.append(predicted) # 将回归训练中得到的预测y值存入列表\n",
    "\n",
    "    # 模型效果评估\n",
    "    metrics_name = [explained_variance_score, mean_absolute_error, mean_squared_error, r2_score] # 回归评估指标对象集\n",
    "    metrics_list = [] # 回归评估指标列表\n",
    "\n",
    "    for i in range(len(mlist)):\n",
    "        tmp = [] # 每个内循环的临时结果列表\n",
    "        for m in metrics_name:\n",
    "            score = m(ytest, y_pred[i]) # 计算每个回归指标结果\n",
    "            tmp.append(score)\n",
    "        metrics_list.append(tmp)\n",
    "\n",
    "    df_cv = pd.DataFrame(cv_scores, index=mnames) # 交叉验证数据框\n",
    "    df_metrics = pd.DataFrame(metrics_list, index=mnames, columns=['ev', 'mae', 'mse', 'r2']) # 回归指标数据框\n",
    "\n",
    "    return df_cv, df_metrics, y_pred, mnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest, scaler2 = data_preprocessing(d.drop('吸阻',axis=1), d.吸阻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_cv, df_metrics, y_pred, mnames = training_model(Xtrain, ytrain, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest, scaler2 = data_preprocessing(d.drop(outlier_x).drop('吸阻',axis=1), d.drop(outlier_x).吸阻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_cv, df_metrics, y_pred, mnames = training_model(Xtrain, ytrain, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(X_train, y_train, model):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train.ravel(), cv=10)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.title('Learning Curves (SVM, RBF kernel, $\\gamma=0.1$)')\n",
    "    plt.ylim(0, 1.01)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR()\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "plot_learning_curve(Xtrain, ytrain, model)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证曲线\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "def plot_validation_curve(X_train, y_train, model, param_name, param_range):\n",
    "    train_scores, test_scores = validation_curve(model, Xtrain, ytrain.ravel(), param_name=param_name, param_range=param_range, cv=10, scoring=\"r2\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve with SVM\")\n",
    "    plt.ylim(0.0, 1.01)\n",
    "    plt.xlabel(\"$\\gamma$\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SVR()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "param_name = 'C'\n",
    "\n",
    "param_range = [1,5,10,15]\n",
    "\n",
    "plot_validation_curve(Xtrain, ytrain, model, param_name, param_range)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymssql\n",
    "\n",
    "\n",
    "# 连接Microsoft SQL Server数据库\n",
    "conn = sqlalchemy.create_engine('mssql+pymssql://sa:sql@localhost/LIKONG?charset=cp936')\n",
    "\n",
    "sql = \"SELECT * FROM table1 WHERE Name LIKE 'SEVO%';\"\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
