{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# 连接MySQL数据库\n",
    "conn = pymysql.connect(host='192.168.1.171', user='root', password='123456', database='jsp')\n",
    "# conn = sqlalchemy.create_engine('mysql+pymysql://root:123456@192.168.1.171/jsp')  # charset=cp936解决了数据库编码为GBK时，数据读取中文乱码的问题\n",
    "\n",
    "sql = 'select * from a'\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cx_Oracle as oracle\n",
    "import os \n",
    "os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8' \n",
    "\n",
    "\n",
    "# 连接Oracle数据库\n",
    "conn = oracle.connect('NNSCUSER','NNSCUSER','192.168.1.171/orcl')\n",
    "conn = sqlalchemy.create_engine(\"oracle+cx_oracle://NNSCUSER:NNSCUSER@192.168.1.171/orcl\")\n",
    "\n",
    "sql=\"select table_name from all_tables where owner='NNSCUSER'\"\n",
    "\n",
    "table_name = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymssql\n",
    "\n",
    "\n",
    "# 连接Microsoft SQL Server数据库\n",
    "# conn = pymssql.connect(server='localhost', user='sa', password='sql', database='tmp')\n",
    "# conn = pymssql.connect(server='localhost', user='', password='', database='tmp', charset='cp936')\n",
    "conn = sqlalchemy.create_engine('mssql+pymssql://sa:sql@localhost/tmp?charset=cp936')  # charset=cp936解决了数据库编码为GBK时，数据读取中文乱码的问题\n",
    "\n",
    "sql = 'select * from params'\n",
    "df = pd.read_sql(sql, conn)\n",
    "df.columns = ['编号', '参数名称', '单位', '最大值', '最小值']\n",
    "df['中心值'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "\n",
    "# 连接Microsoft Access数据库\n",
    "mdb = r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=D:\\test.mdb;'\n",
    "conn = pyodbc.connect(mdb)\n",
    "cur = conn.cursor()\n",
    "\n",
    "for table in cur.tables(tableType='TABLE'):\n",
    "    print(table.table_name)\n",
    "\n",
    "sql = 'SELECT * FROM USERINFO'\n",
    "rows = cur.execute(sql)\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 雷达图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "n = 39\n",
    "a = 0.5*np.random.random(n) + 0.5\n",
    "b = 0.5*np.random.random(n) + 0.5\n",
    "c = (a+b)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['最大值'] = np.where(a>b,a,b)\n",
    "df['最小值'] = np.where(a<b,a,b)\n",
    "df['中心值'] = c\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======设置开始======\n",
    "# plt.rcParams['font.sans-serif'] = ['KaiTi']  # 显示中文\n",
    "#标签\n",
    "labels = df['参数名称']\n",
    "#数据个数\n",
    "dataLenth = len(df)\n",
    "#数据\n",
    "data = np.array(df[['最大值', '最小值', '中心值']])\n",
    "# ======设置结束======\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, dataLenth, endpoint=False)\n",
    "data = np.concatenate((data, [data[0]])) # 闭合\n",
    "angles = np.concatenate((angles, [angles[0]])) # 闭合\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.add_subplot(111, polar=True) # polar参数\n",
    "# plt.polar(angles, data, 'o-', linewidth=1) # 做极坐标系\n",
    "ax.plot(angles, data, 'o-', linewidth=2) # 画线\n",
    "# ax.fill(angles, data, facecolor='r', alpha=0.25) # 填充\n",
    "ax.set_thetagrids(angles*180/np.pi, labels, fontproperties=\"SimHei\")\n",
    "ax.set_title(\"设备运行参数雷达图\", va='bottom', fontproperties=\"SimHei\")\n",
    "ax.set_rlim(0,1)\n",
    "ax.grid(True)\n",
    "plt.legend(labels=('最大值', '最小值', '中心值'), loc='best', bbox_to_anchor=(1.1,1.1),prop={'family': 'SimHei'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机组状态/停机次数、停机时间/机组效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups = pd.DataFrame(['机组1','机组2','机组3','机组4','机组5','机组6','机组7','机组8','机组9','机组10'])\n",
    "status = pd.DataFrame(['运行','停机','运行','停机','运行','停机','运行','停机','运行','停机'])\n",
    "stopcnt = pd.DataFrame([3,2,5,1,6,5,8,3,1,2])\n",
    "stoptime = pd.DataFrame([100,200,120,300,200,320,330,160,220,100])\n",
    "eff = pd.DataFrame([0.8,0.6,0.7,0.5,0.6,0.5,0.5,0.6,0.6,0.8])\n",
    "data = pd.concat([groups,status,stopcnt,stoptime,eff], axis=1)\n",
    "data.columns = ['组名','状态','停机次数','停机时间','效率']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['状态']=='运行']['组名'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['状态']=='停机']['组名'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(data['状态'].value_counts().values,labels=data['状态'].value_counts().index)\n",
    "plt.axis('equal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.plot.bar(figsize=(12,6),rot=0)\n",
    "data.plot.bar(figsize=(12,6),rot=0,secondary_y= '停机时间')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_x, img_y = 28, 28\n",
    "\n",
    "# load the MNIST data set, which already splits into train and test sets for us\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\n",
    "x_train = x_train.reshape(x_train.shape[0], img_x, img_y, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_x, img_y, 1)\n",
    "input_shape = (img_x, img_y, 1)\n",
    "\n",
    "# convert the data to the right type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "history = AccuracyHistory()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[history])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plt.plot(range(1, 11), history.acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常值分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于3sigma的异常数据预警代码\n",
    "\n",
    "n = 3\n",
    "\n",
    "ymean = np.mean(data_y)\n",
    "ystd = np.std(data_y)\n",
    "threshold1 = ymean - n * ystd\n",
    "threshold2 = ymean + n * ystd\n",
    "\n",
    "outlier = [] #将异常值保存\n",
    "outlier_x = []\n",
    "\n",
    "for i in range(0, len(data_y)):\n",
    "    if (data_y[i] < threshold1)|(data_y[i] > threshold2):\n",
    "        outlier.append(data_y[i])\n",
    "        outlier_x.append(data_x[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "outlier = np.round(outlier, 3)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(data_x, data_y)\n",
    "plt.plot(outlier_x, outlier, 'ro')\n",
    "for j in range(len(outlier)):\n",
    "    plt.annotate(outlier[j], xy=(outlier_x[j], outlier[j]), xytext=(outlier_x[j],outlier[j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于箱型图的异常数据预警代码\n",
    "\n",
    "statistics = data_y.describe() #保存基本统计量\n",
    "IQR = statistics.loc['75%']-statistics.loc['25%']   #四分位数间距\n",
    "QL = statistics.loc['25%']  #下四分位数\n",
    "QU = statistics.loc['75%']  #上四分位数\n",
    "threshold1 = QL - 1.5 * IQR #下阈值\n",
    "threshold2 = QU + 1.5 * IQR #上阈值\n",
    "outlier = [] #将异常值保存\n",
    "outlier_x = []\n",
    "\n",
    "for i in range(0, len(data_y)):\n",
    "    if (data_y[i] < threshold1)|(data_y[i] > threshold2):\n",
    "        outlier.append(data_y[i])\n",
    "        outlier_x.append(data_x[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "outlier = np.round(outlier, 3)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(data_x, data_y)\n",
    "plt.plot(outlier_x, outlier, 'ro')\n",
    "for j in range(len(outlier)):\n",
    "    plt.annotate(outlier[j], xy=(outlier_x[j], outlier[j]), xytext=(outlier_x[j],outlier[j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断数据集是否属于正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "x = tb46_dt_mean['WEIGHT_VALUE']\n",
    "k2, p = stats.normaltest(x)\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "if p < alpha:  # 零假设: x来自正态分布\n",
    "    print(\"零假设能被拒绝\")\n",
    "else:\n",
    "    print(\"零假设不能被拒绝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(x, cdf='norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import qqplot\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "fig = qqplot(tb46_dt_mean['WEIGHT_VALUE'], line='q', ax=ax, fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "f = plt.figure(figsize=(12, 8))\n",
    "ax = f.add_subplot(111)\n",
    "probplot(tb46_dt_mean['WEIGHT_VALUE'], plot=ax, rvalue=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def data_preprocessing(x, y):\n",
    "\n",
    "    X = x\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)\n",
    "\n",
    "    scaler1 = MinMaxScaler()\n",
    "\n",
    "    X_scaler1 = scaler1.fit_transform(X)\n",
    "    y_scaler1 = scaler1.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    scaler2 = StandardScaler()\n",
    "\n",
    "    X_scaler2 = scaler2.fit_transform(X)\n",
    "    y_scaler2 = scaler2.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X_scaler2, y_scaler2, test_size=0.2, random_state=666)\n",
    "    \n",
    "    return Xtrain, Xtest, ytrain, ytest, scaler2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso,Ridge,ElasticNet,RANSACRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def training_model(xtrain, ytrain, xtest, ytest):\n",
    "    # 训练回归模型\n",
    "    n_samples, n_features = xtrain.shape # 总样本数，总特征数\n",
    "    n_folds = 6 # 交叉检验次数\n",
    "\n",
    "    lr = LinearRegression() # 线性回归\n",
    "    svr = SVR() # 支持向量机回归\n",
    "    rf = RandomForestRegressor() # 随机森林回归\n",
    "    \"\"\"\n",
    "    n_estimators=20,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=n_features,\n",
    "    random_state=10\n",
    "    \"\"\"\n",
    "    gbr = GradientBoostingRegressor() # GBDT回归\n",
    "    \"\"\"\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=120,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    max_features=n_features,\n",
    "    subsample=1\n",
    "    \"\"\"\n",
    "    knn = KNeighborsRegressor() # K最近邻回归\n",
    "    \"\"\"\n",
    "    n_neighbors=5, \n",
    "    weights='distance', \n",
    "    leaf_size=50\n",
    "    \"\"\"\n",
    "    dt = DecisionTreeRegressor() # 决策树回归\n",
    "    \"\"\"\n",
    "    criterion='mse', \n",
    "    max_depth=None, \n",
    "    max_features=n_features, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, \n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split=2, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    random_state=None, \n",
    "    splitter='best'\n",
    "    \"\"\"\n",
    "    ransac = RANSACRegressor() # 随机采样一致性回归\n",
    "    ridge = Ridge() # Ridge回归\n",
    "    lasso = Lasso() # Lasso回归\n",
    "    en = ElasticNet() # 弹性网络回归\n",
    "\n",
    "    mnames = ['LinearRegression', \n",
    "              'SVR', \n",
    "              'RandomForestRegressor', \n",
    "              'GradientBoostingRegressor', \n",
    "              'KNeighborsRegressor', \n",
    "              'DecisionTreeRegressor', \n",
    "              'RANSACRegressor', \n",
    "              'Ridge', \n",
    "              'Lasso', \n",
    "              'ElasticNet'] # 回归模型名称列表\n",
    "    mlist = [lr, svr, rf, gbr, knn, dt, ransac, ridge, lasso, en] # 回归模型对象列表\n",
    "\n",
    "    cv_scores = [] # 交叉检验结果列表\n",
    "    y_pred = [] # 回归模型预测y值列表\n",
    "\n",
    "    for model in mlist:\n",
    "        scores = cross_val_score(model, xtrain, ytrain.ravel(), cv=n_folds) # 将每个回归模型导入交叉检验\n",
    "        predicted = model.fit(xtrain, ytrain.ravel()).predict(xtest) # 回归训练得到预测y值\n",
    "        cv_scores.append(scores) # 将交叉验证结果存入列表\n",
    "        y_pred.append(predicted) # 将回归训练中得到的预测y值存入列表\n",
    "\n",
    "    # 模型效果评估\n",
    "    metrics_name = [explained_variance_score, mean_absolute_error, mean_squared_error, r2_score] # 回归评估指标对象集\n",
    "    metrics_list = [] # 回归评估指标列表\n",
    "\n",
    "    for i in range(len(mlist)):\n",
    "        tmp = [] # 每个内循环的临时结果列表\n",
    "        for m in metrics_name:\n",
    "            score = m(ytest, y_pred[i]) # 计算每个回归指标结果\n",
    "            tmp.append(score)\n",
    "        metrics_list.append(tmp)\n",
    "\n",
    "    df_cv = pd.DataFrame(cv_scores, index=mnames) # 交叉验证数据框\n",
    "    df_metrics = pd.DataFrame(metrics_list, index=mnames, columns=['ev', 'mae', 'mse', 'r2']) # 回归指标数据框\n",
    "\n",
    "    return df_cv, df_metrics, y_pred, mnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 嵌套交叉验证\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def gridsearch_cv(X_train, y_train, model, params):\n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                      param_grid=params,\n",
    "                      scoring='r2',\n",
    "                      cv=10,\n",
    "                      n_jobs=-1)\n",
    "    scores = cross_val_score(gs, X_train, y_train.ravel(), scoring='r2', cv=5)\n",
    "    return scores\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "params = {'C': param_range, 'epsilon': param_range, 'gamma': param_range}\n",
    "\n",
    "scores = gridsearch_cv(X_train, y_train, model, params)\n",
    "\n",
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习曲线和验证曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(X_train, y_train, model):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train.ravel(), cv=10)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.title('Learning Curves (SVM, RBF kernel, $\\gamma=0.1$)')\n",
    "    plt.ylim(0, 1.01)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "for i,n in enumerate([1,5,10,15]):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plot_learning_curve(X_train, y_train, n)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证曲线\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "def plot_validation_curve(X_train, y_train, model, param_name, param_range):\n",
    "    train_scores, test_scores = validation_curve(model, Xtrain, ytrain.ravel(), param_name=param_name, param_range=param_range, cv=10, scoring=\"r2\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve with SVM\")\n",
    "    plt.ylim(0.0, 1.01)\n",
    "    plt.xlabel(\"$\\gamma$\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "param_range = [1,5,10,15]\n",
    "\n",
    "plot_validation_curve(X_train, y_train, param_range)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
